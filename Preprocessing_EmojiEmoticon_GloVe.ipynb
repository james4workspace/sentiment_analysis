{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Data Import and Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim as gs\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name={\n",
    "    \"emo_extraction\":\"calculate_emo_3\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data\n",
    "def open_pkl_data(file_name):\n",
    "    all_data=dict()\n",
    "    for each_key in file_name.keys():\n",
    "        name = file_name[each_key]\n",
    "        pickle_file = open('DataSet\\\\'+name+\".pkl\",mode=\"rb\")\n",
    "        data = pickle.load(pickle_file)\n",
    "        pickle_file.close()\n",
    "        all_data[each_key]=data\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "emo_path=\"DataSet\\\\emo_collection.csv\"\n",
    "emo_df=pd.read_csv(emo_path,sep=\"|\")\n",
    "#emo_ex = open_pkl_data(file_name)[\"emo_extraction\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emoji/emoticon</th>\n",
       "      <th>key words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ğŸ˜€</td>\n",
       "      <td>face grin grinning face</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ğŸ˜ƒ</td>\n",
       "      <td>face grinning face with big eyes mouth open smile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ğŸ˜„</td>\n",
       "      <td>eye face grinning face with smiling eyes mouth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ğŸ˜</td>\n",
       "      <td>beaming face with smiling eyes eye face grin s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ğŸ˜†</td>\n",
       "      <td>face grinning squinting face laugh mouth satis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2279</th>\n",
       "      <td>(ã€€Â´Ğ”ï½€)ï¾‰(Â´ï½¥Ï‰ï½¥`)ã€€ï¾…ï¾ƒï¾ï¾…ï¾ƒï¾</td>\n",
       "      <td>Patting, nade nade\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2280</th>\n",
       "      <td>(*ï¾Ÿï¾‰Oï¾Ÿ)&lt;ï½µï½µï½µï½µï½«ï½«ï½«ï½«ï½«ï½«ï½«ï½°ï½°ï½°ï½°ï½°ï½²!</td>\n",
       "      <td>Calling out, \"Ooooi!\"\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2281</th>\n",
       "      <td>( ï¾Ÿâˆ€ï¾Ÿ)ï½±ï¾Šï¾Šå…«å…«ï¾‰ãƒ½ï¾‰ãƒ½ï¾‰ãƒ½ï¾‰ ï¼¼ / ï¼¼/ ï¼¼</td>\n",
       "      <td>Evil laugh (literally ahahaHAHA...)\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2282</th>\n",
       "      <td>ï¼ˆãƒ»âˆ€ãƒ» ï¼‰ãƒ¾(- -ï¼›)ã‚³ãƒ©ã‚³ãƒ©</td>\n",
       "      <td>Blaming \"now now\"\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2283</th>\n",
       "      <td>ãŠ(^o^)ã‚„(^O^)ã™(^ï½¡^)ã¿ãƒ(^-^)ï¾‰ï¾</td>\n",
       "      <td>Kana reading \"O ya su mi\" meaning \"Good night\"...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2284 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   emoji/emoticon  \\\n",
       "0                               ğŸ˜€   \n",
       "1                               ğŸ˜ƒ   \n",
       "2                               ğŸ˜„   \n",
       "3                               ğŸ˜   \n",
       "4                               ğŸ˜†   \n",
       "...                           ...   \n",
       "2279        (ã€€Â´Ğ”ï½€)ï¾‰(Â´ï½¥Ï‰ï½¥`)ã€€ï¾…ï¾ƒï¾ï¾…ï¾ƒï¾   \n",
       "2280   (*ï¾Ÿï¾‰Oï¾Ÿ)<ï½µï½µï½µï½µï½«ï½«ï½«ï½«ï½«ï½«ï½«ï½°ï½°ï½°ï½°ï½°ï½²!   \n",
       "2281  ( ï¾Ÿâˆ€ï¾Ÿ)ï½±ï¾Šï¾Šå…«å…«ï¾‰ãƒ½ï¾‰ãƒ½ï¾‰ãƒ½ï¾‰ ï¼¼ / ï¼¼/ ï¼¼   \n",
       "2282            ï¼ˆãƒ»âˆ€ãƒ» ï¼‰ãƒ¾(- -ï¼›)ã‚³ãƒ©ã‚³ãƒ©   \n",
       "2283  ãŠ(^o^)ã‚„(^O^)ã™(^ï½¡^)ã¿ãƒ(^-^)ï¾‰ï¾   \n",
       "\n",
       "                                              key words  \n",
       "0                               face grin grinning face  \n",
       "1     face grinning face with big eyes mouth open smile  \n",
       "2     eye face grinning face with smiling eyes mouth...  \n",
       "3     beaming face with smiling eyes eye face grin s...  \n",
       "4     face grinning squinting face laugh mouth satis...  \n",
       "...                                                 ...  \n",
       "2279                               Patting, nade nade\\n  \n",
       "2280                            Calling out, \"Ooooi!\"\\n  \n",
       "2281              Evil laugh (literally ahahaHAHA...)\\n  \n",
       "2282                                Blaming \"now now\"\\n  \n",
       "2283  Kana reading \"O ya su mi\" meaning \"Good night\"...  \n",
       "\n",
       "[2284 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emo_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "meaning_values=list(emo_df[\"key words\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Extreme Distaste, meant to appear as an exaggerated grimace\\n',\n",
       " 'Shouting\\n',\n",
       " 'Pretending not to notice, asleep because of boredom\\n',\n",
       " 'Kick\\n',\n",
       " 'Discombobulated\\n',\n",
       " 'Running\\n',\n",
       " 'Happy\\n',\n",
       " 'Happy\\n',\n",
       " 'Shocked\\n',\n",
       " 'Really angry\\n',\n",
       " '\"Do it\"\\n',\n",
       " 'Angel\\n',\n",
       " '\"It\\'s here\", Kitaa!, excitement that something has appeared or happened or \"I came\".\\n',\n",
       " 'Girlish version of \"It\\'s here\".\\n',\n",
       " 'Erotic stirring, haa haa\\n',\n",
       " 'Patting, nade nade\\n',\n",
       " 'Calling out, \"Ooooi!\"\\n',\n",
       " 'Evil laugh (literally ahahaHAHA...)\\n',\n",
       " 'Blaming \"now now\"\\n']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meaning_values[-20:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Main Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_transform(input_list,method):\n",
    "    output_list = list()\n",
    "    for text in tqdm(input_list):\n",
    "        input_text = str(text)\n",
    "        new_text = eval(method)(input_text)\n",
    "        output_list.append(new_text)\n",
    "    \n",
    "    return output_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Lower the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_text(input_text):\n",
    "    #lower the words\n",
    "    input_text =input_text.lower()\n",
    "    return input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2284/2284 [00:00<00:00, 176189.77it/s]\n"
     ]
    }
   ],
   "source": [
    "meaning_values1 = fit_transform(meaning_values,method=\"lower_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2284"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(meaning_values1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Seperate the adhered words with space\n",
    " - e.g. \"you?\" ? and you are adhered together, which needs to be split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for assigning emoji, emoticons and punctuations space from each other\n",
    "def assign_space(input_text):\n",
    "    #punctuation\n",
    "    input_text = re.sub(r'[^\\d\\/\\*\\:\\)\\.\\?\\^\\;?\\-_\\'~!\\<\\>\\=\\\"#&$%\\\\\\{\\}\\|\\[\\]Ã§\\+Ï‰â—‹\\@Â¡Ã©Ä±ãƒ»â€¦Â¡\\`ï¼šï¼‰â™¡Ó³ï¼â€œâ€Ã â‰§âˆ‡â‰¦â™‚ÅŸâ‰ˆÂ¬âŠ„â”€âœ”â€¢Ã—Ã¼â€“â‚¹ã€‚Ã³Â°Ê–â€”Â¶Ä·Ã±à¸¿Äºâˆ‘ï¼›â¸](\\!|\\?|\\.+)[^\\d\\/\\*\\:\\)\\.\\?\\^\\;?\\-_\\'~!\\<\\>\\=\\\"#&$%\\\\\\{\\}\\|\\[\\]Ã§\\+Ï‰â—‹\\@Â¡Ã©Ä±ãƒ»â€¦Â¡\\`ï¼šï¼‰â™¡Ó³ï¼â€œâ€Ã â‰§âˆ‡â‰¦â™‚ÅŸâ‰ˆÂ¬âŠ„â”€âœ”â€¢Ã—Ã¼â€“â‚¹ã€‚Ã³Â°Ê–â€”Â¶Ä·Ã±à¸¿Äºâˆ‘ï¼›â¸]',' \\g<0> ' ,input_text)\n",
    "    #replace the redundant space\n",
    "    input_text = re.sub(r'[a-zA-Z]+',r' \\g<0> ',input_text)\n",
    "    input_text = re.sub(r'  +',r' ',input_text)\n",
    "    return input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2284/2284 [00:00<00:00, 47711.21it/s]\n"
     ]
    }
   ],
   "source": [
    "meaning_values2 = fit_transform(meaning_values1,method=\"assign_space\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Remove punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punc(input_text):\n",
    "    new_sentence = input_text\n",
    "    new_sentence = re.sub(r'[^ a-zA-Z]',\"\",new_sentence).strip()\n",
    "    return new_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2284/2284 [00:00<00:00, 103965.43it/s]\n"
     ]
    }
   ],
   "source": [
    "meaning_values3 = fit_transform(meaning_values2,method=\"remove_punc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Correcting spell mistake\n",
    " - e.g. \"nt\" is actually \"not\", \"noo\" is actually \"no\"\n",
    " - in order to do so, here i use library called pyspellchecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker\n",
    "def correct_spell(input_text):\n",
    "    spell = SpellChecker()\n",
    "    words = input_text.split()\n",
    "    new_words=list()\n",
    "    for word in words:\n",
    "        word = spell.correction(word)\n",
    "        new_words.append(word)\n",
    "    sentence = \" \".join(new_words)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2284/2284 [05:15<00:00,  7.23it/s]\n"
     ]
    }
   ],
   "source": [
    "meaning_values4 = fit_transform(meaning_values3,method=\"correct_spell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5. Remove Stopwords made handcrafted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words('english')) # which is not suitable here, coz e.g. in emotion analysis, not happy is opposite from happy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_byhand = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', \n",
    "                    'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', \n",
    "                    'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', \n",
    "                    'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', \n",
    "                    'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', \n",
    "                    'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', \n",
    "                    'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', \n",
    "                    'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', \n",
    "                    'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', \n",
    "                    'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', \n",
    "                    'more', 'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', \n",
    "                    't', 'can', 'will', 'just','now', 'd', 'll', 'm', 'o', 're', 've', 'y']\n",
    "# remove stopwords except those negative words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(input_text):\n",
    "    words = input_text.split()\n",
    "    new_words=list()\n",
    "    for word in words:\n",
    "        if word in stopwords_byhand:\n",
    "            continue\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    sentence = \" \".join(new_words)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2284/2284 [00:00<00:00, 76239.44it/s]\n"
     ]
    }
   ],
   "source": [
    "meaning_values5 = fit_transform(meaning_values4,method=\"remove_stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6. shorthand words translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor_dict= {\n",
    "    }# according to the not_cov, only some certain words which occured only several times need translation, which is unnecessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_words_common(input_text):\n",
    "    'for correcting the words in sentences based on hand-crafted data'\n",
    "    corrected_sentence = \" \"+input_text+\" \"\n",
    "    for each_key in cor_dict.keys():\n",
    "        if each_key in input_text:\n",
    "            rule = \" \"+each_key+\" \"\n",
    "            corrected_sentence = re.sub(rule,\" \"+cor_dict[each_key]+\" \",corrected_sentence)\n",
    "    return corrected_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2284/2284 [00:00<00:00, 126982.19it/s]\n"
     ]
    }
   ],
   "source": [
    "meaning_values6 = fit_transform(meaning_values5,method=\"correct_words_common\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. check data format \n",
    "- for further preprocessing based on pre-trained embedding resource"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. check distinct vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distinct_words(dialogs):\n",
    "    vocab = dict()\n",
    "    for each_dialog in tqdm(dialogs):\n",
    "        text = str(each_dialog)\n",
    "        words_list = text.split(sep=\" \")\n",
    "        for word in words_list:\n",
    "            if word == '':\n",
    "                continue\n",
    "            try:\n",
    "                vocab[word] +=1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2284/2284 [00:00<00:00, 457968.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the length of distinct vocabulary of the version 0 is 3007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vocab0 = distinct_words(meaning_values)\n",
    "print(\"the length of distinct vocabulary of {a} is {b}\".format(a=\"the version 0\",b=len(vocab0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2284/2284 [00:00<00:00, 325378.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the length of distinct vocabulary of the version 1 is 2940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vocab1 = distinct_words(meaning_values1)\n",
    "print(\"the length of distinct vocabulary of {a} is {b}\".format(a=\"the version 1\",b=len(vocab1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2284/2284 [00:00<00:00, 286280.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the length of distinct vocabulary of the version 2 is 2803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vocab2 = distinct_words(meaning_values2)\n",
    "print(\"the length of distinct vocabulary of {a} is {b}\".format(a=\"the version 2\",b=len(vocab2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2284/2284 [00:00<00:00, 568904.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the length of distinct vocabulary of the version 3 is 2707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vocab3 = distinct_words(meaning_values3)\n",
    "print(\"the length of distinct vocabulary of {a} is {b}\".format(a=\"the version 3\",b=len(vocab3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2284/2284 [00:00<00:00, 457990.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the length of distinct vocabulary of the version 4 is 2686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vocab4 = distinct_words(meaning_values4)\n",
    "print(\"the length of distinct vocabulary of {a} is {b}\".format(a=\"the version 4\",b=len(vocab4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2284/2284 [00:00<00:00, 572577.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the length of distinct vocabulary of the version 5 is 2638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vocab5 = distinct_words(meaning_values5)\n",
    "print(\"the length of distinct vocabulary of {a} is {b}\".format(a=\"the version 5\",b=len(vocab5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2284/2284 [00:00<00:00, 567422.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the length of distinct vocabulary of the version 6 is 2638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vocab6 = distinct_words(meaning_values6)\n",
    "print(\"the length of distinct vocabulary of {a} is {b}\".format(a=\"the version 6\",b=len(vocab6)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. check the percentage of words in data can be processed by GloVe pre-trained data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import processed words list from glove\n",
    "GloVe_path=\"DataSet\\\\glove.840B.300d_words.pkl\"\n",
    "def open_pkl(path):\n",
    "    pickle_file = open(path,mode='rb')\n",
    "    data = pickle.load(pickle_file)\n",
    "    pickle_file.close()\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "glo_words=open_pkl(GloVe_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[',', '.', 'the', 'and', 'to', 'of', 'a', 'in', '\"', ':']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glo_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the function for check the coverage of the vocabulary and the text\n",
    "import operator\n",
    "def check_coverage(vocab, words_list):\n",
    "    cov_vocab =0\n",
    "    num_vocab = len(vocab)\n",
    "    cov_text = 0\n",
    "    not_cov = dict()\n",
    "    not_text = 0\n",
    "    for word in tqdm(vocab):\n",
    "        if word in words_list:\n",
    "            cov_vocab += 1\n",
    "            cov_text += vocab[word]\n",
    "        else:\n",
    "            not_cov[word]=vocab[word]\n",
    "            not_text += vocab[word]\n",
    "            pass\n",
    "    percent_cov_vocab = cov_vocab/num_vocab\n",
    "    percent_cov_text = cov_text/(cov_text+not_text)\n",
    "    print(\"In Embedding Index we have {:.2%} coverage of distinct vocabulary\".format(percent_cov_vocab))\n",
    "    print(\"And we have {:.2%} coverage of all text\".format(percent_cov_text))\n",
    "    sorted_not_cov = sorted(not_cov.items(),key= operator.itemgetter(1),reverse = True)\n",
    "    print(\"The number of words which are not covered in word2vec resource is: {0}\".format(len(sorted_not_cov)))\n",
    "    return sorted_not_cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3007/3007 [00:09<00:00, 322.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Embedding Index we have 89.92% coverage of distinct vocabulary\n",
      "And we have 84.50% coverage of all text\n",
      "The number of words which are not covered in word2vec resource is: 303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# original data\n",
    "not_cov0 = check_coverage(vocab0, glo_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('flag:', 261),\n",
       " ('family:', 25),\n",
       " ('woman,', 24),\n",
       " ('man,', 22),\n",
       " ('Laughing,', 20),\n",
       " ('out,', 18),\n",
       " ('Sad,', 18),\n",
       " ('crying\\n', 18),\n",
       " ('cheeky/playful,', 17),\n",
       " ('laugh\\n', 15)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_cov0[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2940/2940 [00:09<00:00, 296.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Embedding Index we have 89.66% coverage of distinct vocabulary\n",
      "And we have 84.40% coverage of all text\n",
      "The number of words which are not covered in word2vec resource is: 304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# lower the words\n",
    "not_cov1 = check_coverage(vocab1, glo_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('flag:', 261),\n",
       " ('sad,', 31),\n",
       " ('family:', 25),\n",
       " ('woman,', 24),\n",
       " ('man,', 22),\n",
       " ('laughing,', 20),\n",
       " ('out,', 18),\n",
       " ('crying\\n', 18),\n",
       " ('cheeky/playful,', 17),\n",
       " ('embarrassed,', 15),\n",
       " ('fish\\n', 15),\n",
       " ('laugh\\n', 15),\n",
       " ('keycap:', 13),\n",
       " ('frown,', 13),\n",
       " ('angry,', 13),\n",
       " ('surprise,', 13),\n",
       " ('shock,', 13),\n",
       " ('nervous,', 13),\n",
       " ('skeptical,', 12),\n",
       " ('annoyed,', 12)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_cov1[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2803/2803 [00:03<00:00, 800.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Embedding Index we have 98.14% coverage of distinct vocabulary\n",
      "And we have 96.14% coverage of all text\n",
      "The number of words which are not covered in word2vec resource is: 52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# assign space to seperate adhered words\n",
    "not_cov2 = check_coverage(vocab2, glo_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('\\n', 301),\n",
       " ('âŠ›', 7),\n",
       " ('dogeza', 7),\n",
       " ('\"\\n', 7),\n",
       " ('despai', 5),\n",
       " ('.\\n', 5),\n",
       " ('intercardinal', 4),\n",
       " ('baltan', 3),\n",
       " ('merwoman', 2),\n",
       " ('ğŸ¤·', 2),\n",
       " ('\",', 2),\n",
       " ('!\"\\n', 2),\n",
       " ('\".\\n', 2),\n",
       " ('tichel', 1),\n",
       " ('merperson', 1),\n",
       " ('vicu', 1),\n",
       " ('orthoptera', 1),\n",
       " ('Ç', 1),\n",
       " ('molusc', 1),\n",
       " ('jeotgarak', 1)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_cov2[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2707/2707 [00:02<00:00, 976.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Embedding Index we have 99.22% coverage of distinct vocabulary\n",
      "And we have 99.56% coverage of all text\n",
      "The number of words which are not covered in word2vec resource is: 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# remove punctuations\n",
    "not_cov3 = check_coverage(vocab3, glo_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dogeza', 7),\n",
       " ('despai', 5),\n",
       " ('intercardinal', 4),\n",
       " ('baltan', 3),\n",
       " ('merwoman', 2),\n",
       " ('tichel', 1),\n",
       " ('merperson', 1),\n",
       " ('vicu', 1),\n",
       " ('orthoptera', 1),\n",
       " ('molusc', 1),\n",
       " ('jeotgarak', 1),\n",
       " ('kuaizi', 1),\n",
       " ('hocho', 1),\n",
       " ('moyai', 1),\n",
       " ('withershins', 1),\n",
       " ('aesculapius', 1),\n",
       " ('clipperton', 1),\n",
       " ('czechia', 1),\n",
       " ('eswatini', 1),\n",
       " ('deflagged', 1)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_cov3[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2686/2686 [00:02<00:00, 1153.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Embedding Index we have 99.52% coverage of distinct vocabulary\n",
      "And we have 99.81% coverage of all text\n",
      "The number of words which are not covered in word2vec resource is: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# correct misspell\n",
    "not_cov4 = check_coverage(vocab4, glo_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('intercardinal', 4),\n",
       " ('hijaz', 1),\n",
       " ('meyerson', 1),\n",
       " ('unaco', 1),\n",
       " ('orthoptera', 1),\n",
       " ('hoylake', 1),\n",
       " ('falae', 1),\n",
       " ('jeotgarak', 1),\n",
       " ('withershins', 1),\n",
       " ('aesculapius', 1),\n",
       " ('bovet', 1),\n",
       " ('clapperton', 1),\n",
       " ('eswatini', 1)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_cov4[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2638/2638 [00:02<00:00, 1079.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Embedding Index we have 99.51% coverage of distinct vocabulary\n",
      "And we have 99.80% coverage of all text\n",
      "The number of words which are not covered in word2vec resource is: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# remove stopwords\n",
    "not_cov5 = check_coverage(vocab5, glo_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2638/2638 [00:02<00:00, 1123.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Embedding Index we have 99.51% coverage of distinct vocabulary\n",
      "And we have 99.80% coverage of all text\n",
      "The number of words which are not covered in word2vec resource is: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# give shorthand a translation\n",
    "not_cov6 = check_coverage(vocab6, glo_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. save new dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dict = dict()\n",
    "final_dict[\"emoji/emoticon\"]=list(emo_df[\"emoji/emoticon\"])\n",
    "final_dict[\"key words\"]=meaning_values6\n",
    "final_df = pd.DataFrame.from_dict(final_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emoji/emoticon</th>\n",
       "      <th>key words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ğŸ˜€</td>\n",
       "      <td>face grin grinning face</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ğŸ˜ƒ</td>\n",
       "      <td>face grinning face big eyes mouth open smile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ğŸ˜„</td>\n",
       "      <td>eye face grinning face smiling eyes mouth ope...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ğŸ˜</td>\n",
       "      <td>beaming face smiling eyes eye face grin smile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ğŸ˜†</td>\n",
       "      <td>face grinning squinting face laugh mouth sati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2279</th>\n",
       "      <td>(ã€€Â´Ğ”ï½€)ï¾‰(Â´ï½¥Ï‰ï½¥`)ã€€ï¾…ï¾ƒï¾ï¾…ï¾ƒï¾</td>\n",
       "      <td>patting made made</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2280</th>\n",
       "      <td>(*ï¾Ÿï¾‰Oï¾Ÿ)&lt;ï½µï½µï½µï½µï½«ï½«ï½«ï½«ï½«ï½«ï½«ï½°ï½°ï½°ï½°ï½°ï½²!</td>\n",
       "      <td>calling ooooh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2281</th>\n",
       "      <td>( ï¾Ÿâˆ€ï¾Ÿ)ï½±ï¾Šï¾Šå…«å…«ï¾‰ãƒ½ï¾‰ãƒ½ï¾‰ãƒ½ï¾‰ ï¼¼ / ï¼¼/ ï¼¼</td>\n",
       "      <td>evil laugh literally ahahahaha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2282</th>\n",
       "      <td>ï¼ˆãƒ»âˆ€ãƒ» ï¼‰ãƒ¾(- -ï¼›)ã‚³ãƒ©ã‚³ãƒ©</td>\n",
       "      <td>blaming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2283</th>\n",
       "      <td>ãŠ(^o^)ã‚„(^O^)ã™(^ï½¡^)ã¿ãƒ(^-^)ï¾‰ï¾</td>\n",
       "      <td>kana reading ya su mi meaning good night night</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2284 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   emoji/emoticon  \\\n",
       "0                               ğŸ˜€   \n",
       "1                               ğŸ˜ƒ   \n",
       "2                               ğŸ˜„   \n",
       "3                               ğŸ˜   \n",
       "4                               ğŸ˜†   \n",
       "...                           ...   \n",
       "2279        (ã€€Â´Ğ”ï½€)ï¾‰(Â´ï½¥Ï‰ï½¥`)ã€€ï¾…ï¾ƒï¾ï¾…ï¾ƒï¾   \n",
       "2280   (*ï¾Ÿï¾‰Oï¾Ÿ)<ï½µï½µï½µï½µï½«ï½«ï½«ï½«ï½«ï½«ï½«ï½°ï½°ï½°ï½°ï½°ï½²!   \n",
       "2281  ( ï¾Ÿâˆ€ï¾Ÿ)ï½±ï¾Šï¾Šå…«å…«ï¾‰ãƒ½ï¾‰ãƒ½ï¾‰ãƒ½ï¾‰ ï¼¼ / ï¼¼/ ï¼¼   \n",
       "2282            ï¼ˆãƒ»âˆ€ãƒ» ï¼‰ãƒ¾(- -ï¼›)ã‚³ãƒ©ã‚³ãƒ©   \n",
       "2283  ãŠ(^o^)ã‚„(^O^)ã™(^ï½¡^)ã¿ãƒ(^-^)ï¾‰ï¾   \n",
       "\n",
       "                                              key words  \n",
       "0                              face grin grinning face   \n",
       "1         face grinning face big eyes mouth open smile   \n",
       "2      eye face grinning face smiling eyes mouth ope...  \n",
       "3        beaming face smiling eyes eye face grin smile   \n",
       "4      face grinning squinting face laugh mouth sati...  \n",
       "...                                                 ...  \n",
       "2279                                 patting made made   \n",
       "2280                                     calling ooooh   \n",
       "2281                    evil laugh literally ahahahaha   \n",
       "2282                                           blaming   \n",
       "2283    kana reading ya su mi meaning good night night   \n",
       "\n",
       "[2284 rows x 2 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"DataSet\\\\emo_collection_glove.csv\"\n",
    "final_df.to_csv(path,sep='|',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
